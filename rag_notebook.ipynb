{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Custom RAG System (Gemini Flash)\n",
                "\n",
                "## Phase 1: Environment & Project Setup\n",
                "\n",
                "This phase ensures:\n",
                "- Clean project structure\n",
                "- Isolated Python environment\n",
                "- Secure secrets management\n",
                "- Reproducible execution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "print(\"Python executable:\", sys.executable)\n",
                "print(\"Python version:\", sys.version)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import platform\n",
                "print(\"OS:\", platform.system())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()\n",
                "\n",
                "print(\"GOOGLE_API_KEY loaded:\", bool(os.getenv(\"GOOGLE_API_KEY\")))\n",
                "print(\"PINECONE_API_KEY loaded:\", bool(os.getenv(\"PINECONE_API_KEY\")))\n",
                "print(\"PINECONE_ENV loaded:\", bool(os.getenv(\"PINECONE_ENV\")))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Phase 1 setup complete and verified.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 2: Document Ingestion & Text Extraction\n",
                "\n",
                "This phase:\n",
                "- Loads PDFs from disk\n",
                "- Extracts page-level text\n",
                "- Preserves metadata (source + page)\n",
                "- Performs validation and sanity checks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "from langchain.document_loaders import PyPDFLoader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "PDF_DIR = Path(\"data/pdfs\")\n",
                "assert PDF_DIR.exists(), \"data/pdfs folder does not exist\"\n",
                "\n",
                "pdf_files = sorted(PDF_DIR.glob(\"*.pdf\"))\n",
                "\n",
                "print(f\"Found {len(pdf_files)} PDF files:\")\n",
                "for f in pdf_files:\n",
                "    print(\"-\", f.name)\n",
                "\n",
                "assert len(pdf_files) > 0, \"No PDFs found. Add files to data/pdfs/\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "documents = []\n",
                "\n",
                "for pdf_path in pdf_files:\n",
                "    print(f\"\\nLoading: {pdf_path.name}\")\n",
                "    loader = PyPDFLoader(str(pdf_path))\n",
                "    pages = loader.load()\n",
                "\n",
                "    print(f\"  Pages extracted: {len(pages)}\")\n",
                "    documents.extend(pages)\n",
                "\n",
                "print(f\"\\nTotal pages loaded from all PDFs: {len(documents)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sample = documents[0]\n",
                "print(\"Metadata:\", sample.metadata)\n",
                "print(\"\\n--- Content Preview (first 1000 chars) ---\\n\")\n",
                "print(sample.page_content[:1000])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "empty_pages = [i for i, d in enumerate(documents) if len(d.page_content.strip()) < 50]\n",
                "\n",
                "print(\"Total pages:\", len(documents))\n",
                "print(\"Empty or near-empty pages:\", len(empty_pages))\n",
                "\n",
                "if len(empty_pages) > 0:\n",
                "    print(\"Indices of empty pages:\", empty_pages[:10])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "total_chars = sum(len(d.page_content) for d in documents)\n",
                "avg_chars = total_chars / len(documents)\n",
                "\n",
                "print(\"Total characters:\", total_chars)\n",
                "print(\"Average characters per page:\", int(avg_chars))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Phase 2 complete: Documents ingested and validated.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 3: Text Normalization & Chunking\n",
                "\n",
                "This phase:\n",
                "- Normalizes raw extracted text\n",
                "- Removes formatting noise\n",
                "- Prepares semantically meaningful chunks\n",
                "- Preserves source metadata for traceability"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "from langchain.text_splitter import RecursiveCharacterTextSplitter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def normalize_text(text: str) -> str:\n",
                "    # Collapse multiple newlines\n",
                "    text = re.sub(r\"\\n+\", \"\\n\", text)\n",
                "\n",
                "    # Collapse excessive whitespace\n",
                "    text = re.sub(r\"\\s+\", \" \", text)\n",
                "\n",
                "    # Strip edges\n",
                "    text = text.strip()\n",
                "\n",
                "    return text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for doc in documents:\n",
                "    doc.page_content = normalize_text(doc.page_content)\n",
                "\n",
                "print(\"Text normalization complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=== NORMALIZED TEXT SAMPLE ===\\n\")\n",
                "print(documents[0].page_content[:1000])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text_splitter = RecursiveCharacterTextSplitter(\n",
                "    chunk_size=1000,\n",
                "    chunk_overlap=150,\n",
                "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "chunks = text_splitter.split_documents(documents)\n",
                "\n",
                "print(\"Total pages:\", len(documents))\n",
                "print(\"Total chunks created:\", len(chunks))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sample_chunk = chunks[0]\n",
                "\n",
                "print(\"Chunk metadata:\", sample_chunk.metadata)\n",
                "print(\"\\n--- Chunk preview ---\\n\")\n",
                "print(sample_chunk.page_content[:1000])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sizes = [len(c.page_content) for c in chunks]\n",
                "\n",
                "print(\"Min chunk size:\", min(sizes))\n",
                "print(\"Max chunk size:\", max(sizes))\n",
                "print(\"Avg chunk size:\", sum(sizes) // len(sizes))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Phase 3 complete: Text normalized and chunked successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 4: Embedding Generation (Sentence Transformers)\n",
                "\n",
                "This phase:\n",
                "- Initializes a HuggingFace embedding model\n",
                "- Converts text chunks into vector embeddings\n",
                "- Verifies embedding shape and consistency"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sentence_transformers import SentenceTransformer\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
                "\n",
                "print(\"Loading embedding model:\", EMBEDDING_MODEL_NAME)\n",
                "embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
                "\n",
                "print(\"Model loaded.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "texts = [chunk.page_content for chunk in chunks]\n",
                "print(\"Total chunks to embed:\", len(texts))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "embeddings = embedder.encode(\n",
                "    texts,\n",
                "    batch_size=32,\n",
                "    show_progress_bar=True,\n",
                "    convert_to_numpy=True,\n",
                "    normalize_embeddings=True\n",
                ")\n",
                "\n",
                "print(\"Embeddings generated.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Embeddings array shape:\", embeddings.shape)\n",
                "print(\"Single embedding dimension:\", embeddings.shape[1])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Sample embedding (first 10 values):\")\n",
                "print(embeddings[0][:10])\n",
                "print(\"Vector norm:\", np.linalg.norm(embeddings[0]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "assert embeddings.shape[0] == len(chunks), \"Mismatch between chunks and embeddings!\"\n",
                "assert embeddings.shape[1] == 384, \"Unexpected embedding dimension!\"\n",
                "\n",
                "print(\"Embedding consistency checks passed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Phase 4 complete: Embeddings generated and validated.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (rag-venv)",
            "language": "python",
            "name": "rag-venv"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}