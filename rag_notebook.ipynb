{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Custom RAG System (Gemini Flash)\n",
                "\n",
                "## Phase 1: Environment & Project Setup\n",
                "\n",
                "This phase ensures:\n",
                "- Clean project structure\n",
                "- Isolated Python environment\n",
                "- Secure secrets management\n",
                "- Reproducible execution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "print(\"Python executable:\", sys.executable)\n",
                "print(\"Python version:\", sys.version)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import platform\n",
                "print(\"OS:\", platform.system())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()\n",
                "\n",
                "print(\"GOOGLE_API_KEY loaded:\", bool(os.getenv(\"GOOGLE_API_KEY\")))\n",
                "print(\"PINECONE_API_KEY loaded:\", bool(os.getenv(\"PINECONE_API_KEY\")))\n",
                "print(\"PINECONE_ENV loaded:\", bool(os.getenv(\"PINECONE_ENV\")))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Phase 1 setup complete and verified.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 2: Document Ingestion & Text Extraction\n",
                "\n",
                "This phase:\n",
                "- Loads PDFs from disk\n",
                "- Extracts page-level text\n",
                "- Preserves metadata (source + page)\n",
                "- Performs validation and sanity checks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "from langchain.document_loaders import PyPDFLoader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "PDF_DIR = Path(\"data/pdfs\")\n",
                "assert PDF_DIR.exists(), \"data/pdfs folder does not exist\"\n",
                "\n",
                "pdf_files = sorted(PDF_DIR.glob(\"*.pdf\"))\n",
                "\n",
                "print(f\"Found {len(pdf_files)} PDF files:\")\n",
                "for f in pdf_files:\n",
                "    print(\"-\", f.name)\n",
                "\n",
                "assert len(pdf_files) > 0, \"No PDFs found. Add files to data/pdfs/\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "documents = []\n",
                "\n",
                "for pdf_path in pdf_files:\n",
                "    print(f\"\\nLoading: {pdf_path.name}\")\n",
                "    loader = PyPDFLoader(str(pdf_path))\n",
                "    pages = loader.load()\n",
                "\n",
                "    print(f\"  Pages extracted: {len(pages)}\")\n",
                "    documents.extend(pages)\n",
                "\n",
                "print(f\"\\nTotal pages loaded from all PDFs: {len(documents)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sample = documents[0]\n",
                "print(\"Metadata:\", sample.metadata)\n",
                "print(\"\\n--- Content Preview (first 1000 chars) ---\\n\")\n",
                "print(sample.page_content[:1000])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "empty_pages = [i for i, d in enumerate(documents) if len(d.page_content.strip()) < 50]\n",
                "\n",
                "print(\"Total pages:\", len(documents))\n",
                "print(\"Empty or near-empty pages:\", len(empty_pages))\n",
                "\n",
                "if len(empty_pages) > 0:\n",
                "    print(\"Indices of empty pages:\", empty_pages[:10])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "total_chars = sum(len(d.page_content) for d in documents)\n",
                "avg_chars = total_chars / len(documents)\n",
                "\n",
                "print(\"Total characters:\", total_chars)\n",
                "print(\"Average characters per page:\", int(avg_chars))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Phase 2 complete: Documents ingested and validated.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 3: Text Normalization & Chunking\n",
                "\n",
                "This phase:\n",
                "- Normalizes raw extracted text\n",
                "- Removes formatting noise\n",
                "- Prepares semantically meaningful chunks\n",
                "- Preserves source metadata for traceability"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "from langchain.text_splitter import RecursiveCharacterTextSplitter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def normalize_text(text: str) -> str:\n",
                "    # Collapse multiple newlines\n",
                "    text = re.sub(r\"\\n+\", \"\\n\", text)\n",
                "\n",
                "    # Collapse excessive whitespace\n",
                "    text = re.sub(r\"\\s+\", \" \", text)\n",
                "\n",
                "    # Strip edges\n",
                "    text = text.strip()\n",
                "\n",
                "    return text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for doc in documents:\n",
                "    doc.page_content = normalize_text(doc.page_content)\n",
                "\n",
                "print(\"Text normalization complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=== NORMALIZED TEXT SAMPLE ===\\n\")\n",
                "print(documents[0].page_content[:1000])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text_splitter = RecursiveCharacterTextSplitter(\n",
                "    chunk_size=1000,\n",
                "    chunk_overlap=150,\n",
                "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "chunks = text_splitter.split_documents(documents)\n",
                "\n",
                "print(\"Total pages:\", len(documents))\n",
                "print(\"Total chunks created:\", len(chunks))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sample_chunk = chunks[0]\n",
                "\n",
                "print(\"Chunk metadata:\", sample_chunk.metadata)\n",
                "print(\"\\n--- Chunk preview ---\\n\")\n",
                "print(sample_chunk.page_content[:1000])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sizes = [len(c.page_content) for c in chunks]\n",
                "\n",
                "print(\"Min chunk size:\", min(sizes))\n",
                "print(\"Max chunk size:\", max(sizes))\n",
                "print(\"Avg chunk size:\", sum(sizes) // len(sizes))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Phase 3 complete: Text normalized and chunked successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 4: Embedding Generation (Sentence Transformers)\n",
                "\n",
                "This phase:\n",
                "- Initializes a HuggingFace embedding model\n",
                "- Converts text chunks into vector embeddings\n",
                "- Verifies embedding shape and consistency"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sentence_transformers import SentenceTransformer\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
                "\n",
                "print(\"Loading embedding model:\", EMBEDDING_MODEL_NAME)\n",
                "embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
                "\n",
                "print(\"Model loaded.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "texts = [chunk.page_content for chunk in chunks]\n",
                "print(\"Total chunks to embed:\", len(texts))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "embeddings = embedder.encode(\n",
                "    texts,\n",
                "    batch_size=32,\n",
                "    show_progress_bar=True,\n",
                "    convert_to_numpy=True,\n",
                "    normalize_embeddings=True\n",
                ")\n",
                "\n",
                "print(\"Embeddings generated.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Embeddings array shape:\", embeddings.shape)\n",
                "print(\"Single embedding dimension:\", embeddings.shape[1])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Sample embedding (first 10 values):\")\n",
                "print(embeddings[0][:10])\n",
                "print(\"Vector norm:\", np.linalg.norm(embeddings[0]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "assert embeddings.shape[0] == len(chunks), \"Mismatch between chunks and embeddings!\"\n",
                "assert embeddings.shape[1] == 384, \"Unexpected embedding dimension!\"\n",
                "\n",
                "print(\"Embedding consistency checks passed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Phase 4 complete: Embeddings generated and validated.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 5: Vector Database (Pinecone Serverless)\n",
                "\n",
                "This phase:\n",
                "- Initializes Pinecone Serverless client\n",
                "- Creates a new serverless index (if needed)\n",
                "- Batches and upserts vectors with metadata"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from pinecone import Pinecone, ServerlessSpec\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
                "assert PINECONE_API_KEY is not None, \"Missing PINECONE_API_KEY\"\n",
                "\n",
                "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
                "print(\"Pinecone client initialized.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "INDEX_NAME = \"notebooklm-rag-antigravity\"\n",
                "\n",
                "existing_indexes = [i[\"name\"] for i in pc.list_indexes()]\n",
                "\n",
                "if INDEX_NAME not in existing_indexes:\n",
                "    print(\"Creating new serverless index:\", INDEX_NAME)\n",
                "    pc.create_index(\n",
                "        name=INDEX_NAME,\n",
                "        dimension=embeddings.shape[1],\n",
                "        metric=\"cosine\",\n",
                "        spec=ServerlessSpec(\n",
                "            cloud=\"aws\",\n",
                "            region=\"us-east-1\"\n",
                "        )\n",
                "    )\n",
                "else:\n",
                "    print(\"Using existing index:\", INDEX_NAME)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "index = pc.Index(INDEX_NAME)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vectors = []\n",
                "\n",
                "for i, (chunk, vector) in enumerate(zip(chunks, embeddings)):\n",
                "    vectors.append({\n",
                "        \"id\": f\"chunk-{i}\",\n",
                "        \"values\": vector.tolist(),\n",
                "        \"metadata\": {\n",
                "            \"source\": chunk.metadata.get(\"source\", \"\"),\n",
                "            \"page\": chunk.metadata.get(\"page\", \"\"),\n",
                "            \"text\": chunk.page_content[:1000]  # metadata size safety\n",
                "        }\n",
                "    })\n",
                "\n",
                "print(\"Prepared vectors:\", len(vectors))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "BATCH_SIZE = 100\n",
                "\n",
                "for i in range(0, len(vectors), BATCH_SIZE):\n",
                "    batch = vectors[i:i+BATCH_SIZE]\n",
                "    index.upsert(vectors=batch)\n",
                "    print(f\"Uploaded {min(i + BATCH_SIZE, len(vectors))} / {len(vectors)} vectors\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "stats = index.describe_index_stats()\n",
                "print(\"Index stats:\", stats)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Phase 5 complete: Vectors successfully stored in Pinecone Serverless.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (rag-venv)",
            "language": "python",
            "name": "rag-venv"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}