{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Custom RAG System (Gemini Flash)\n",
                "\n",
                "## Phase 1: Environment & Project Setup\n",
                "\n",
                "This phase ensures:\n",
                "- Clean project structure\n",
                "- Isolated Python environment\n",
                "- Secure secrets management\n",
                "- Reproducible execution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "print(\"Python executable:\", sys.executable)\n",
                "print(\"Python version:\", sys.version)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import platform\n",
                "print(\"OS:\", platform.system())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()\n",
                "\n",
                "print(\"GOOGLE_API_KEY loaded:\", bool(os.getenv(\"GOOGLE_API_KEY\")))\n",
                "print(\"PINECONE_API_KEY loaded:\", bool(os.getenv(\"PINECONE_API_KEY\")))\n",
                "print(\"PINECONE_ENV loaded:\", bool(os.getenv(\"PINECONE_ENV\")))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Phase 1 setup complete and verified.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 2: Document Ingestion & Text Extraction\n",
                "\n",
                "This phase:\n",
                "- Loads PDFs from disk\n",
                "- Extracts page-level text\n",
                "- Preserves metadata (source + page)\n",
                "- Performs validation and sanity checks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "from langchain.document_loaders import PyPDFLoader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "PDF_DIR = Path(\"data/pdfs\")\n",
                "assert PDF_DIR.exists(), \"data/pdfs folder does not exist\"\n",
                "\n",
                "pdf_files = sorted(PDF_DIR.glob(\"*.pdf\"))\n",
                "\n",
                "print(f\"Found {len(pdf_files)} PDF files:\")\n",
                "for f in pdf_files:\n",
                "    print(\"-\", f.name)\n",
                "\n",
                "assert len(pdf_files) > 0, \"No PDFs found. Add files to data/pdfs/\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "documents = []\n",
                "\n",
                "for pdf_path in pdf_files:\n",
                "    print(f\"\\nLoading: {pdf_path.name}\")\n",
                "    loader = PyPDFLoader(str(pdf_path))\n",
                "    pages = loader.load()\n",
                "\n",
                "    print(f\"  Pages extracted: {len(pages)}\")\n",
                "    documents.extend(pages)\n",
                "\n",
                "print(f\"\\nTotal pages loaded from all PDFs: {len(documents)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sample = documents[0]\n",
                "print(\"Metadata:\", sample.metadata)\n",
                "print(\"\\n--- Content Preview (first 1000 chars) ---\\n\")\n",
                "print(sample.page_content[:1000])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "empty_pages = [i for i, d in enumerate(documents) if len(d.page_content.strip()) < 50]\n",
                "\n",
                "print(\"Total pages:\", len(documents))\n",
                "print(\"Empty or near-empty pages:\", len(empty_pages))\n",
                "\n",
                "if len(empty_pages) > 0:\n",
                "    print(\"Indices of empty pages:\", empty_pages[:10])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "total_chars = sum(len(d.page_content) for d in documents)\n",
                "avg_chars = total_chars / len(documents)\n",
                "\n",
                "print(\"Total characters:\", total_chars)\n",
                "print(\"Average characters per page:\", int(avg_chars))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Phase 2 complete: Documents ingested and validated.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 3: Text Normalization & Chunking\n",
                "\n",
                "This phase:\n",
                "- Normalizes raw extracted text\n",
                "- Removes formatting noise\n",
                "- Prepares semantically meaningful chunks\n",
                "- Preserves source metadata for traceability"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "from langchain.text_splitter import RecursiveCharacterTextSplitter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def normalize_text(text: str) -> str:\n",
                "    # Collapse multiple newlines\n",
                "    text = re.sub(r\"\\n+\", \"\\n\", text)\n",
                "\n",
                "    # Collapse excessive whitespace\n",
                "    text = re.sub(r\"\\s+\", \" \", text)\n",
                "\n",
                "    # Strip edges\n",
                "    text = text.strip()\n",
                "\n",
                "    return text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for doc in documents:\n",
                "    doc.page_content = normalize_text(doc.page_content)\n",
                "\n",
                "print(\"Text normalization complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=== NORMALIZED TEXT SAMPLE ===\\n\")\n",
                "print(documents[0].page_content[:1000])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text_splitter = RecursiveCharacterTextSplitter(\n",
                "    chunk_size=1000,\n",
                "    chunk_overlap=150,\n",
                "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "chunks = text_splitter.split_documents(documents)\n",
                "\n",
                "print(\"Total pages:\", len(documents))\n",
                "print(\"Total chunks created:\", len(chunks))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sample_chunk = chunks[0]\n",
                "\n",
                "print(\"Chunk metadata:\", sample_chunk.metadata)\n",
                "print(\"\\n--- Chunk preview ---\\n\")\n",
                "print(sample_chunk.page_content[:1000])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sizes = [len(c.page_content) for c in chunks]\n",
                "\n",
                "print(\"Min chunk size:\", min(sizes))\n",
                "print(\"Max chunk size:\", max(sizes))\n",
                "print(\"Avg chunk size:\", sum(sizes) // len(sizes))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Phase 3 complete: Text normalized and chunked successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 4: Embedding Generation (Sentence Transformers)\n",
                "\n",
                "This phase:\n",
                "- Initializes a HuggingFace embedding model\n",
                "- Converts text chunks into vector embeddings\n",
                "- Verifies embedding shape and consistency"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sentence_transformers import SentenceTransformer\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
                "\n",
                "print(\"Loading embedding model:\", EMBEDDING_MODEL_NAME)\n",
                "embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
                "\n",
                "print(\"Model loaded.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "texts = [chunk.page_content for chunk in chunks]\n",
                "print(\"Total chunks to embed:\", len(texts))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "embeddings = embedder.encode(\n",
                "    texts,\n",
                "    batch_size=32,\n",
                "    show_progress_bar=True,\n",
                "    convert_to_numpy=True,\n",
                "    normalize_embeddings=True\n",
                ")\n",
                "\n",
                "print(\"Embeddings generated.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Embeddings array shape:\", embeddings.shape)\n",
                "print(\"Single embedding dimension:\", embeddings.shape[1])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Sample embedding (first 10 values):\")\n",
                "print(embeddings[0][:10])\n",
                "print(\"Vector norm:\", np.linalg.norm(embeddings[0]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "assert embeddings.shape[0] == len(chunks), \"Mismatch between chunks and embeddings!\"\n",
                "assert embeddings.shape[1] == 384, \"Unexpected embedding dimension!\"\n",
                "\n",
                "print(\"Embedding consistency checks passed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Phase 4 complete: Embeddings generated and validated.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 5: Vector Database (Pinecone Serverless)\n",
                "\n",
                "This phase:\n",
                "- Initializes Pinecone Serverless client\n",
                "- Creates a new serverless index (if needed)\n",
                "- Batches and upserts vectors with metadata"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from pinecone import Pinecone, ServerlessSpec\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
                "assert PINECONE_API_KEY is not None, \"Missing PINECONE_API_KEY\"\n",
                "\n",
                "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
                "print(\"Pinecone client initialized.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "INDEX_NAME = \"notebooklm-rag-antigravity\"\n",
                "\n",
                "existing_indexes = [i[\"name\"] for i in pc.list_indexes()]\n",
                "\n",
                "if INDEX_NAME not in existing_indexes:\n",
                "    print(\"Creating new serverless index:\", INDEX_NAME)\n",
                "    pc.create_index(\n",
                "        name=INDEX_NAME,\n",
                "        dimension=embeddings.shape[1],\n",
                "        metric=\"cosine\",\n",
                "        spec=ServerlessSpec(\n",
                "            cloud=\"aws\",\n",
                "            region=\"us-east-1\"\n",
                "        )\n",
                "    )\n",
                "else:\n",
                "    print(\"Using existing index:\", INDEX_NAME)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "index = pc.Index(INDEX_NAME)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vectors = []\n",
                "\n",
                "for i, (chunk, vector) in enumerate(zip(chunks, embeddings)):\n",
                "    vectors.append({\n",
                "        \"id\": f\"chunk-{i}\",\n",
                "        \"values\": vector.tolist(),\n",
                "        \"metadata\": {\n",
                "            \"source\": chunk.metadata.get(\"source\", \"\"),\n",
                "            \"page\": chunk.metadata.get(\"page\", \"\"),\n",
                "            \"text\": chunk.page_content[:1000]  # metadata size safety\n",
                "        }\n",
                "    })\n",
                "\n",
                "print(\"Prepared vectors:\", len(vectors))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "BATCH_SIZE = 100\n",
                "\n",
                "for i in range(0, len(vectors), BATCH_SIZE):\n",
                "    batch = vectors[i:i+BATCH_SIZE]\n",
                "    index.upsert(vectors=batch)\n",
                "    print(f\"Uploaded {min(i + BATCH_SIZE, len(vectors))} / {len(vectors)} vectors\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "stats = index.describe_index_stats()\n",
                "print(\"Index stats:\", stats)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Phase 5 complete: Vectors successfully stored in Pinecone Serverless.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 6: Semantic Retrieval Layer\n",
                "\n",
                "This phase:\n",
                "- Encodes user queries\n",
                "- Performs similarity search in Pinecone\n",
                "- Retrieves top-k relevant chunks\n",
                "- Allows inspection of retrieved context"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def embed_query(query: str) -> np.ndarray:\n",
                "    vec = embedder.encode(\n",
                "        [query],\n",
                "        convert_to_numpy=True,\n",
                "        normalize_embeddings=True\n",
                "    )\n",
                "    return vec[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def retrieve_top_k(query: str, k: int = 5):\n",
                "    query_vec = embed_query(query)\n",
                "\n",
                "    result = index.query(\n",
                "        vector=query_vec.tolist(),\n",
                "        top_k=k,\n",
                "        include_metadata=True\n",
                "    )\n",
                "\n",
                "    return result[\"matches\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "TEST_QUERY = \"What is the main topic discussed in these documents?\"\n",
                "\n",
                "results = retrieve_top_k(TEST_QUERY, k=5)\n",
                "\n",
                "print(\"Retrieved chunks:\", len(results))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for i, match in enumerate(results, 1):\n",
                "    print(f\"\\n--- Result {i} ---\")\n",
                "    print(\"Score:\", match[\"score\"])\n",
                "    print(\"Source:\", match[\"metadata\"].get(\"source\"))\n",
                "    print(\"Page:\", match[\"metadata\"].get(\"page\"))\n",
                "    print(\"\\nText Preview:\\n\")\n",
                "    print(match[\"metadata\"].get(\"text\")[:1000])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(results) > 0, \"No retrieval results returned!\"\n",
                "print(\"Retrieval sanity check passed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Phase 6 complete: Semantic retrieval layer operational.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 7: RAG Answer Generation (Gemini Flash)\n",
                "\n",
                "This phase:\n",
                "- Integrates Gemini Flash as the LLM\n",
                "- Combines semantic retrieval + generation\n",
                "- Uses strict grounding to prevent hallucination\n",
                "- Produces user-facing answers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import google.generativeai as genai\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()\n",
                "\n",
                "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
                "assert GOOGLE_API_KEY is not None, \"Missing GOOGLE_API_KEY\"\n",
                "\n",
                "genai.configure(api_key=GOOGLE_API_KEY)\n",
                "\n",
                "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
                "print(\"Gemini Flash initialized.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_context(retrieved_matches, max_chars=6000):\n",
                "    context_blocks = []\n",
                "    total_chars = 0\n",
                "\n",
                "    for match in retrieved_matches:\n",
                "        text = match[\"metadata\"].get(\"text\", \"\")\n",
                "        source = match[\"metadata\"].get(\"source\", \"\")\n",
                "        page = match[\"metadata\"].get(\"page\", \"\")\n",
                "\n",
                "        block = f\"[Source: {source}, Page: {page}]\\n{text}\\n\"\n",
                "        \n",
                "        if total_chars + len(block) > max_chars:\n",
                "            break\n",
                "\n",
                "        context_blocks.append(block)\n",
                "        total_chars += len(block)\n",
                "\n",
                "    return \"\\n---\\n\".join(context_blocks)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_rag_prompt(context: str, question: str) -> str:\n",
                "    prompt = f\"\"\"\n",
                "You are a factual assistant.\n",
                "\n",
                "Answer the question strictly using ONLY the context below.\n",
                "If the answer is not present in the context, say:\n",
                "\"I don't have enough information in the provided documents.\"\n",
                "\n",
                "CONTEXT:\n",
                "{context}\n",
                "\n",
                "QUESTION:\n",
                "{question}\n",
                "\n",
                "ANSWER:\n",
                "\"\"\"\n",
                "    return prompt.strip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def rag_answer(query: str, k: int = 5):\n",
                "    # Step 1: Retrieve relevant chunks\n",
                "    retrieved = retrieve_top_k(query, k=k)\n",
                "\n",
                "    # Step 2: Build context window\n",
                "    context = build_context(retrieved)\n",
                "\n",
                "    # Step 3: Build grounded prompt\n",
                "    prompt = build_rag_prompt(context, query)\n",
                "\n",
                "    # Step 4: Generate answer using Gemini\n",
                "    response = model.generate_content(prompt)\n",
                "\n",
                "    return {\n",
                "        \"query\": query,\n",
                "        \"answer\": response.text,\n",
                "        \"context\": context,\n",
                "        \"retrieved\": retrieved\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "TEST_QUESTION = \"Summarize the main topics discussed in these documents.\"\n",
                "\n",
                "result = rag_answer(TEST_QUESTION, k=5)\n",
                "\n",
                "print(\"QUESTION:\\n\", result[\"query\"])\n",
                "print(\"\\nANSWER:\\n\", result[\"answer\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n=== CONTEXT USED FOR ANSWER ===\\n\")\n",
                "print(result[\"context\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\ud83d\udd0e Interactive Q&A Mode (type 'exit' to quit)\\n\")\n",
                "\n",
                "while True:\n",
                "    user_question = input(\"Ask a question: \").strip()\n",
                "\n",
                "    if user_question.lower() in [\"exit\", \"quit\"]:\n",
                "        print(\"Exiting Q&A.\")\n",
                "        break\n",
                "\n",
                "    if len(user_question) < 3:\n",
                "        print(\"Please ask a more specific question.\")\n",
                "        continue\n",
                "\n",
                "    print(\"\\n\ud83d\udd0d Searching documents and generating answer...\\n\")\n",
                "\n",
                "    result = rag_answer(user_question, k=5)\n",
                "\n",
                "    print(\"\u2753 QUESTION:\")\n",
                "    print(user_question)\n",
                "\n",
                "    print(\"\\n\u2705 ANSWER:\")\n",
                "    print(result[\"answer\"])\n",
                "\n",
                "    print(\"\\n\ud83d\udcda SOURCES USED:\")\n",
                "    for i, match in enumerate(result[\"retrieved\"], 1):\n",
                "        meta = match[\"metadata\"]\n",
                "        print(f\"{i}. {meta.get('source')} (page {meta.get('page')})\")\n",
                "\n",
                "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Phase 7 complete: End-to-end RAG system operational with Gemini Flash.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (rag-venv)",
            "language": "python",
            "name": "rag-venv"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}